---
layout: post
title: 梯度下降
categories: ML
author: Kuang
tags: optimization

---

在机器学习中，梯度下降是十分常见的一种方法。在很多机器学习算法中，对于参数的学习调整，采用的方法都是梯度下降。可以说，梯度下降是每一个学习Machine Learning 的人必须彻底掌握的方法。只有理解了梯度下降，才能初步对于机器学习有所了解。




接下来，这篇博客将从头到尾详细讲述一遍梯度下降。首先必须说明的是，这篇博客是参照Andrew Ng讲解的《Machine Learning》课程中[关于梯度下降的部分][1]，不愿意看我博客的可以直接去看视频。

### 代价函数

在学习梯度下降之前，我们首先要知道什么是代价函数。假设我们的模型是$h_\theta(x)=\theta_0+\theta_1x$，其中，$\theta_0$和$\theta_1$就是所谓的“参数”,$x$就是我们的输入，也就是我们的训练集或测试集的特征。所谓的“学习”,就是要找到合适的$\theta_0$和$\theta_1$,让模型的输出“越接近”真实值越好。那么问题来了，怎么才叫“接近”？这时候我们可以定义一个Cost Function，这个Cost Function可以理解为计算模型的输出和真实值之间的“接近”程度，显然，我们想要这个Cost Function越小越好。代价函数可以有很多种，在这里我以$L=\sum_ {i=1}^{m}(h_\theta(x_i)-y_i)^2$作为例子，“训练模型”需要做的事情就是使得$L$越小越好。这里的$m$是样本的数目。（实际应用中，为了把乘法计算化为加法，往往会取对数，当然这不是讨论的zhong'di）

### 梯度下降

知道了我们的目标(使代价函数尽可能小)后，我们应该考虑的就是如何找到最合适的参数$\theta_0$，$\theta_1$ ;梯度下降就是一种让我们找到最合适的参数的方法。
可以设想，整个“学习”的过程是这样的：

1. 初始化参数 $\theta_0$ ，$\theta_1$
2. 调整 $\theta_0$ ，$\theta_1$的值，使得$L$减小
3. 重复步骤2直至$L$的大小达到我们的要求

我们可以使用**梯度下降**来实现步骤2
如图所示
![GradiantDescent.PNG-315.4kB][2]
图像的z轴就是代价函数的值，我们的目标就是找到“最低”的那个点。想象一下我们在这片“山岭”的任意位置，如何才能走到最低的点？显然只要往“下”走就可以了，那么如何才能最快走到最低的点呢？这时候我们想起来微积分中梯度的概念，显然梯度方向下降最快。于是我们可以给出参数$\theta_0$，$\theta_1$的更新公式：$$\theta_j:=\theta_j - \alpha\frac{\partial}{\partial\theta_j}(\theta_0,\theta_1)$$
最后需要说明一下$\alpha$这个参数，我们称$\alpha$为学习率，也叫“步长”，想象一下，如果步长太小，我们很容易就能走到山岭的“谷底”，但是这片山岭有很多“谷底”，我们走到的那个不一定是最低点。这种现象就是我们常说的“陷入局部最优”；如果步长过大，我们可能在谷底附近徘徊多次，也就是所谓的“收敛速度慢”，甚至无法收敛。因此我们必须要找到一个合适的$\alpha$，这也是这里唯一需要手动调整的参数。


  [1]: https://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent
  [2]: http://static.zybuluo.com/kuangjun/fzh5zrogw2mdyza85xuie4be/GradiantDescent.PNG